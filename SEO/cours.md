# Search Engine Optimization

**Syphaïwong Bay**  
*syphaiwong@assonance.net*

Références à lire :
=====
+ Dominique Cardon, *La démocratie Internet. Promesses et limites*, Seuil, coll. « La république des idées », 2010, 102 p. [BEC : RICHELIEU[DOCQ20] (1).]

Objectifs du cours :
======
- comprendre ce que c'est le référencement naturel
- connaître suffisamment le domaine pour comprendre comment intéragir avec des conseillers de référencement naturel en milieu pro.

## Cours du 4/12/2017
Objectifs
======
+ qu'est-ce que c'est que le RN (Référencement naturel)
+ enjeux du RN
+ organisation des projets
+ anatomie d'une page de résultats
+ importance de l'intention de recherche
+ requêtes de recherche
+ enjeux juridiques


Définitions
=======
**SEO** : un acronyme courant pour désigner le référencement naturel.
Le référencement naturel (ou référencement organique) s'oppose à la publicité.
Le référencement gratuit s'oppose au référencement payant (ex: annonces ; payant parce que c'est de la publicité).  

**Wikipédia** définit le SEO comme "un ensemble de techniques visant à favoriser la compréhension de la thématique et du contenu d'une ou de l'ensemble des pages d'un site Web par les moteurs de recherche".  
**Jeff JARVIS**, *La Méthode Google*, Pocket, 2011 : "Si on ne peut pas vous chercher, on ne vous trouvera pas".

On parle beaucoup de Google parce que Google représente près de 90% des recherches faites en ligne. Mais il y a d'autres moteurs de recherche qu'il ne faut bien sûr pas ignorer puisque ce sont quand même des millions de personnes.

Plus de la moitié des utilisateurs de moteurs de recherche ne font pas la distinction entre un **résultat payant** et un **résultat naturel**. C'est notamment pour ça que la publicité fonctionne si bien...  

*Google* et *Yahoo* ne sont donc **pas des moteurs de vérité**. Ils rendent compte d'un état du web, voire même d'un état du référencement du web.

Google est une entreprise privée, dont les objectifs sont lucratifs. La majorité des revenus provient de la publicité. En développant des services gratuits pour ses utilisateurs et des projets annexes (ex: *Google Cultural Institute*), Google affirme sa position de leader, gagne des parts de marché et se montre comme une régie publicitaire inévitable. Yahoo, Bing (Microsoft), c'est pareil.

Le métier
======
Le référencement naturel est un véritable métier qui s'articule autour de trois axes :
+ **technique :** il s'agit de structurer et de concevoir un site internet pour que les robots les comprennes (les robots de Google sonde internet selon un principe de sérendipité).
+ ** contenu :** il s'agit d'apporter les éléments de réponse dans sa thématique pour être une référence.
+ **popularité :** Le site doit donc avoir des **citations** (c'est-à-dire qu'il y a des liens vers ce site depuis d'autres sites).

Le référencement naturel doit être présent dès la génèse d'un site internet : le SEO n'est pas une matière à part, et il n'est pas l'ennemi des autres métiers. Il faut penser le référencement à chaque étape : le référencement ne sert pas seulement à attirer vers soi des vue : il sert aussi sa propre image, et est un critère de qualité.  

Plusieurs éléments doivent être pensés ou analysés :
+ sitemap.xml
+ concurrence
+ objectifs SEO
+ analyses sémantiques
+ contenu
+ robots.txt
+ maillage interne
+ accessibilité robot
+ Google Search Console

Cycle de vie & SEO : `cf diapo`.

Le référencement est un travail itératif et actif. Même si on sait ce que veut Google en terme de critères/normes, on ne sait pas exactement comment il classe les sites. L'algorithme de Google est secret, il faut donc répéter des actions pour voir leurs effets.

Le **leviers d'acquisition** incluent le SEO. Ils sont une porte d'entrée pour accéder aux contenus. Il y a d'autres leviers d'acquisition : `affichage dans l'espace public`, `télévision`, `réseaux sociaux`, `référencement payant (SEA)`.

----
Il y a beaucoup de noms de poste différents :
+ consultant SEO / référencement naturel
+ chargé SEO
+ Traffic manager
+ directeurs marketing
+ responsable numérique

Il y a plusieurs spécialités pour le métier :
+ sémantique, éditorial
+ vitesse de chargement, performances
+ structure du site
+ analyse des logs serveurs
+ analytics
+ netlinking, popularité
+ ...

La base de connaissance est la même, mais l'attention n'est pas portée sur les mêmes éléments, et les tâches appliquées ne sont pas les mêmes d'une spécialité à l'autre.  

----

C'est un métier qui demande des qualités. Il est impératif de connaître le fonctionnement d'un moteur de recherche, et les différents étapes de conception d'un site Internet. Le référencement est un travail itératif. Il faut des qualités indispensables : des compétences techniques qui se rattachent à ce que l'on fait, mais aussi une connaissance de la manière dont le référencement fonctionne. Il faut être conscient et à l'écoute des objectifs et des enjeux du site sur lequel on travail.
Il faut savoir prendre en compte les besoins réels. Un consultant ne cherche pas nécessairement à avoir *beaucoup* de visites, mais les *bonnes* visites.  

----
Le moteur apporte les résultats les plus satisfaisant en comprenant quelle est **l'intention de recherche**. Les résultats sont donc classés par pertinence à partir de la recherche.

Vu la diversification des services de moteur de recherche, les bases de données dernières ne sont pas les mêmes. Les sources de trafic via le référencement naturel sont donc multiplié :  
+ Google Images
+ Google actualités
+ etc.

On peut s'amuser à comparer les résultats donnés par chacun des services du moteur de recherche.

En moyenne une page de résultat donne 10 **résultats organiques**. Mais pour les **requêtes concurrentielles**, qui donne lieu à beaucoup de trafic et beaucoup de concurrence avec des résultats payants, cela peut baisser.

`knowledge graph` : c'est un bloc qui apparaît à la droite et qui donne les informations essentiels. Lancé il y a quelques années, à partir d'un partenariat avec Wikipédia et d'autres bases de données. Google a dépassé à ce moment-là le stade d'indexation pur pour basculer vers l'encyclopédie.

La **position zéro** sur la page de résultat, c'est un premier résultat qui n'est pas tout à fait un résultat organique mais une extraction d'informations qui correspondent le plus à la requête.

La formultation des requêtes
=====
Les requêtes mobiles et les requêtes sur ordinateurs ne sont pas tout à fait les mêmes.
Google a annoncé qu'il base sur les contenus "mobile" d'un site web pour classer les pages dans son index. Avoir un site mobile n'est plus une option sous peine de voir votre visibilité totalement réduite dans les résultats de recherche. On parle de tendance "mobile first". Il est donc essentiel d'avoir des sites responsive. Ce n'est pas nécessairement une bonne pratique de supprimer des éléments sur la mise en page mobile : peut-être qu'ils ne sont pas utiles pour la version desktop.

__Syntaxe :__
+ `site:domaine.com` : permet d'afficher les pages d'un site qui sont classées et on peut voir les pages qui sont les plus populaires.

Bonnes pratiques
====
Google et de nombreux navigateurs donnent des exemples de bonnes pratiques pour avoir un référencement le plus efficace possible.
Il y a des moteurs alternatifs, qui permettent notamment de ne pas être espionnés :
+ **duckduckgo** (ce n'est en fait pas un véritable moteur de recherche mais un méta-moteur de recherche qui agrège les résultats de Google)
+ **qwant :** c'est un moteur français, développé en France avec ses propres algorithmes. Il a un fonctionnement différents de Google. Qwant propose un "qwant" pour les enfants -> "qwant junior".

Fonctionnement d'un moteur de recherche
====
Avant de pouvoir afficher ses pages de résultats (`SERP` (Search Engine Result Page ?)) sur les demandes des internautes, Google doit enregistrer des pages web dans son moteur. Lorsqu'un robot de moteur de recherche consulte mon site, il visite le site en entier, pas forcément en commençant par la page d'accueil. Il faut donc concevoir son site comme visitable par toutes les pages possibles.  

Google suit trois étapes :
+ découverte : les robots parcourent le web de lien en liens (*surfeur aléatoire*)
+ stockage : stockage des données dans leur index
+ re-crawl : découverte de nouveaux liens

Il existe des **serpomètres** qui permettent de consulter/mesure la manière dont les pages de résultats changent (il arrive qu'il y ait des remaniement dans les SERP).  
Le référencement Google pénalise la duplication de contenus sur un site.

L'intention de recherche
====
Compréhension des requêtes de recherche et les résultats en réponse.

Une même thématiques peut être décliné en plusieurs requêtes. En comprenant l'intention d'une requête, le moteur de recherche est capable d'apporter des réponses différentes.
Des outils de suivi de référencement permettent de suivre les mots-clefs utilisés pour des SERP dans lesquelles un site donné apparaît. C'est au SEO/Référenceur de s'assurer que le site apparaît au moins sur les requêtes du public visé (ex par rapport aux services proposés par le site ou l'organisme derrière le site). Ensuite, on élargit les résultats sémantiques.

`https://answerthepublic.com/` permet de connaître des mot-clefs les plus utilisés avec telle ou telle requêtes.

Théorie de la longue traîne appliquée au SEO
======
C'est une théorie popularisée par Chris Anderson en 2004 (*Wired*). La présence sur les mot-clefs moins concurrentiels peut apporter un volume de visites égal ou supérieur à celui sur les mots-clefs difficiles à atteindre. Plus on est présent sur les mots-clefs plus longs en élargissant vos champs sémantiques, plus vous devenez pertinents et pourrez vous positionner sur les mots-clefs courts. C'est la stratégie d'Amazon.   

On vise normalement les requêtes courtes et pertinentes, et ensuite on élargit sont champs sémantique.

Le référencement naturel au plan juridiques
====
Il y a des enjeux légaux derrière le SEO.  
Le référencement naturel est soumis aux problématiques de concurrence. Les moteurs de recherche mettent à disposition des consignes et des règles pour guider les créateurs de site, mais *ils ne dictent pas les lois*. Les actions réalisées en ligne doivent avant tout être en cohésion avec les lois du pays pour lequel il est destiné.  

Exemple d'*Interflora* : pénalisé pendant plusieurs semaine par Google pour une campagne de publicité anti-concurrentielle -> est-ce que c'était vraiment à Google de s'occuper de pénaliser le site ? C'était en réalité plutôt au pays de régler ce problème légal.

La loi a une influence directe sur la gestion des sites :
+ droit au déréférencement (ou "droit à l'oubli") : https://www.cnil.fr/fr/le-droit-au-dereferencement
+ Bandeau d'acceptation de cookie obligatoire :
+ ...

Pratiques anti-concurrentielle :
Chercher à se positionner soi-même sur les requêtes liées précisément à la marque d'un concurrent, vouloir détourner son trafic, ou mener des actions pour porter atteinte à ses résultats, sont des pratiques anti-concurrentielles punies par la loi. On peut être accusé de parasitisme économique, concurrence déloyale, dénigrement commercial. Il est admis que le client d'un prestataire de SEO est responsable de ses pratiques : c'est donc important de bien choisir son prestataire (réputation, etc).

Propriété intellectuelle et industrielle :
sur le web on s'expose aussi aux problématiques de contrefaçons, de plagiat. [cf fin du diapo]


## Cours 2 : 11 décembre 2017
Cours plus pratique cette semaine.  

Tous les moteurs de recherche proposent des guidelines pour optimiser le référencement d'un site.

`onsite` / `offsite` : deux termes pour différencier certaines pratiques. Google a deux algo qui permettent d'évaluer le comportement d'un site : pinguin (offiste) ou panda (onsite). -> "spammy"

Faire du SEO ou acheter une prestation en référencement, c'est vouloir se rendre plus visible sur Internet par le biais des moteurs de recherche.
Cela peut se faire en différentes étapes :
- identifier ce que recherchent les internautes sur Google  
définir une stratégie de contenus pour répondre à ces requêtes
- évaluer la performance des éléments déployés
- et recommencer (parce qu'il peut y avoir une saisonnalité des recherches menées, des comportements des internautes et de leurs attentes, des variations).

### Le spam et Google
spam = contenu de mauvaise qualité créé dans le seul but d'attirer des internautes depuis les moteurs de recherche.
C'est une vraie plaie pour Google. Le traitement du spam en général représente 0.2% des émissions de gaz à effet de sert mondiales.  

### Les mots-clefs :
Les mots-clefs sont les objectifs de visibilité. Il faut identifier les mot-clefs auxquels un site se rattache (y compris des mot-clefs qui correspondent à des erreurs : ex "école des chart**r**es"). Il faut les classer : différentes catégories ex : marque, type de produits, services, activités, localisation, ...

#### Long tail :
Il y a 3 sections de mots-clefs, si on reprend la théorie de la longue tail :
- short tail : requêtes courtes, mais avec des combinaisons de mot-clefs qui donnent énormément de résultat, concurrentiels.
- middle tail : zone intermédiaire.
- long tail : les requêtes sont plus longues, plus précises, mais rarement les plus utilisés, elles génèrent des résultats peu nombreux mais généralement pertinent.

C'est une bonne chose de segmenter ses objectifs selon les sections de la long-tail : sur la short tail c'est pas nécessaire de faire beaucoup d'effort puisque c'est des efforts vains. Il faut calculer ses objectifs en fonction de ça. Sur la longtail il est nécessaire d'arriver en premier, sinon ça veut dire qu'il y à un problème sur le site. Sur le middle tail il faut mettre ses efforts. Ensemble, la middle tail et la long tail rassemblent autant de visites que la short tail.

### site mapping
La page d'accueil, est une `page de niveau 1`.  
Toutes les pages après la page d'accueil sont des `pages profondes`.
La structuration des contenus aide le référencement, aussi bien que l'internaute dans sa navigation. Les pages sont liées entre elles grâce aux liens que l'on réalise. Il faut faire attention aux liens que l'on fait, car chaque lien est un lien qui sera potentiellement utilisé par les robots de Google. Il y a un risque que ce soient des pages qui donnent des informations techniques sur le site.  
+ Google Search Console : une application où n'importe qui peut inscrire son site pour analyser les flux sur son site.
+ la requête `site:nomdusite.com` permet de circonscrire une requête à une site, et du coup de faire apparaître dans le moteur de recherche les pages selon la hiérarchie du moteur de recherche, pour voir les pages qui sont les plus "populaires".

[...]

### Données structurées
- les meta keywords ont été tellement manipulé à de mauvaises fins, que les moteurs de recherche ne les prennent plus en compte.
- le balisage sémantique permet de rendre compte de la structure d'une page. Un site dont la structure sémantique est claire sera toujours mieux référencé. En référencement, la bonne pratique c'est de n'avoir d'un seul h1 par page. Sur les sites de presses, on ne respecte pas toujours cela. Ce sont des erreurs fréquentes.

La règle d'or du SEO c'est une page  = un contenu = une URL unique;

`copyscape :` permet de détecter les contenus similaires à un site, voire identiques. Il y a aussi `siteliner.com` (gratuit) qui analyse la duplication interne du contenu.

### Duplication de contenus
Google préconise de faire des URL canoniques : elles sont placées dans le head. Elle donne l'URL de la page originale d'où provient le contenu. Il ne faut pas en abuser, mais c'est légitime dans certains cas que les contenus se ressemblent beaucoup. Ce sont des équivalent de citation de source, mais en assez bancale. On utilise en général ça pour fixer des problème plus qu'en préalable. Ça dévalorise un peu le contenu.

### Les Images
Les images sont une source réelle de trafic. Il ne faut pas les négliger. Google ne "lit" pas les images : ce qu'il fait c'est qu'il analyse l'aspect globale de l'image (structures, couleurs, etc). C'est l'attribut `alt` qui permet d'inscrire la description d'une image. C'est intéressant pour les moteurs de recherche puisqu'ils comprennent de quoi parle l'image. Le nom du fichier influe aussi sur le positionnement de l'image dans le référencement. A priori ça marche bien de mettre des mots-clefs essentiels pour décrire l'image dans le nom du fichier. + accessibilité.

Pour le référencement naturel, on dit en général que le contenu est roi. Google est complètement aveugle : il ne comprend pas les vidéos, les images et les scripts. Il ne lit que le texte. C'est important de le savoir parce que ça influence les stratégies de référencement.  

Les mots clefs qui sont utilisés dans les balises méta, devraient aussi être utilisés dans le contenu de la page. Sinon il y a un problème d'adéquation. Il ne faut pas non plus sur-optimiser : mettre en gras chaque occurrence d'un mot-clef par exemple... C'est généralement pénalisé par Google (on peut aussi le dénoncer) et penser aussi à la navigation. La sur-optimisation (keyword stuffing par ex) c'est aussi visible sur une page et ça génère de la méfiance de la part du navigateur.

### Le maillage interne
Les liens entre les pages d'un site crée le maillage internet et assure l'unité du site. Il y a des liens ascendants, qui suivent l'arborescence des pages. Il faut aussi faire du "cross-linking" qui permet de favoriser le fait que le robot navigue sur les différentes pages pour les prendre en compte et les analyser. Bien sûr avec un contexte. Même principe : cross-selling : quand on vient pour acheter une chose et qu'on repart avec d'autres choses.

Les robots de Google ont un "budget temps" (crawl budget) : il y a donc un temps limité accordé à chaque site. `PageSpeed` permet de tester la "rapidité" d'un site (temps de chargement des contenus, etc). Pour les moteurs de recherche, c'est un critère d'analyse : un site est pertinent quand il est aussi performant. En plus, plus vite le site est chargé, plus le robot peut explorer de nombreuses parties du site.

### Question des langues
Avoir un site en plusieurs langues nécessite de les séparer clairement, et de faire comprendre à Google dans quelle langue est chaque page. -> Une URL = une contenu unique = un contenu totalement traduit.
Spécifier cela peut avoir un impact sur la structure du site et donc son crawl par Google. .fr et .be : même si c'est le même contenu, le public visé n'est pas le même et ça limite l'effet de plagiat. Plus, il faut savoir que pour un domaine particulier, des lois différentes peuvent s'appliquer. .fr est soumis à la loi française (?). Spécifier la langue d'un site, et son domaine, c'est notamment une manière de s'assurer qu'on enfreint pas une législation.

### HTTPS
C'est un protocole qui permet de sécuriser les données qui circulent sur le Web. Google a beaucoup communiqué sur le fait que l'utilisation du protocole HTTPS est valorisé. Mais cela risque de générer des "migrations" : copie du contenu HTTP vers même contenu en HTPPS, avec redirections. Inutile de se lancer dans un projet de migration seulement pour le SEO. Mais pour des raisons de sécurité, ou à l'occasion de la refonte du design d'un site, ça vaut le coup.

### Résumé des bonnes pratiques :
- <head> : titre, méta description, données structurées
- <body> : balisage sémantique (HN), contenu unique, images
- une URL unique pour chaque contenu
- un contenu de qualité dans une seule langue
- des liens entre les pages
- un site rapide, compatible mobile, des technologies compatibles avec le SEO.  

### Outils supplémentaires
- robot.txt
- sitemap.xml

Deux éléments qui sont mis à la racine du site et qui s'adressent aux robots de référencement. Robot.txt permet notamment de limiter la navigation des robots en déterminant des pages autorisées et des pages non-autorisées.  
Le sitemap fait une liste des url du site, avec la date de la dernière modification, ... . Il fonctionne comme une liste blanche qui donne à Google toutes les pages pour "gagner du temps" par rapport aux robots. Attention parce que cette liste n'est pas exclusive : Google peut trouver les pages autrement, y compris des pages qui ne seraient pas dans le sitemap.


Il faut faire preuve de bon sens, et appliquer les règles que nous avons vues : pour un bon référencement, il faut qu'il y ait du texte et que Google soit capable de comprendre ce qu'il y a dedans.

### Analyses de trafic
Avoir un historique du trafic sur plusieurs années permet de prendre en compte la saisonnalité dans l'évaluation des évolutions de trafic. `Google analytics` est un outil très puissant mais aussi très vaste.
Il faut définir son "indice de performance" en fonction de son objectif : si c'est d'avoir plus de visiteurs physiques, alors c'est les informations pratiques sur lesquels ont souhaite avoir plus de visites. Permet de mettre en corrélation des éléments qui peuvent avoir une influence l'un sur l'autre.
L'Analytics est un domaine spécifique.

Objectifs à définir :
- quelle est la fonction de votre site ?
- quel service doit-il apporter ? (si un site est bien fait : chaque page propose un service)
- Quelles actions j'attends de l'internaute ? (par rapport à un service, on s'attend à une action de la part de l'internaute ; on peut aussi comparer ce qu'il se passe en ligne et ce qu'on attend IRL ensuite (ex inscription à une événement))
- Ai-je plusieurs cibles ?

Les données de trafic peuvent permettre de définir sa stratégie. Exemple : permet de définir le meilleur moment pour publier un nouveau contenu : ex fort taux de visite en début d'après-midi : il faut avoir prévu d'avoir le contenu en ligne et disponible pour ce moment-là. On peut programmer la publication de contenus en fonction du temps que ça prend pour qu'il soit correctement en ligne (ex en pleine nuit). Remarque : en France on a tendance à oublier que les gens vont sur internet le midi et le week-end : il faut pas hésiter à programmer de la publication.).

Les `canaux d'acquisition` permettre de savoir d'où viennent les visiteurs : pour savoir comment mieux viser d'autres parts d'internautes. Possible de savoir sur quels support les navigations se font : si beaucoup sur mobile, il faut optimiser la navigation sur mobile. On peut aussi connaître la tranche d'âge : pour optimiser son site en fonction d'un public spécifique (image, design, contenu...).  

Ce sont des données pragmatiques, mais elle ne dirigent pas l'action. C'est au SEO de prendre des décisions avant tout, les données peuvent juste servir à comprendre dans quel contexte ces décisions sont prises.

### Reputation
Comment contrôler l'image de soi ? Google ne contrôle pas les informations. Les moteurs de recherche agrègent et classe les résultats pour apporter  les réponses les plus satisfaisantes. On n'a pas forcément envie que tout s'affiche.  

Actualités, publication, images, vidéos, etc -> autant d'éléments à surveiller !

Le référencement peut être un outil pour sonder la réputation. Certaines personnes sont spécialisés dans la gestion de crise/badbuzz et utilisent le référencement pour pallier cela.

En SEO une "marque" c'est toute sorte de nom
